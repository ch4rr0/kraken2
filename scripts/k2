#!/usr/bin/env python3

import argparse
import ftplib
import glob
import os
import re
import sys
import gzip
import inspect
import shlex
import subprocess
import shutil
import urllib
import urllib.error
import urllib.parse
import urllib.request
import tempfile
import tarfile

LIBRARY_DIR="/tmp"
NCBI_SERVER="ftp.ncbi.nlm.nih.gov"
FTP_SERVER="ftp://" + NCBI_SERVER
WRAPPER_ARGS_TO_BIN_ARGS = {
    "block_size"            : "-B",
    "classified_out"        : "-C",
    "confidence"            : "-T",
    "fast_build"            : "-F",
    "kmer_len"              : "-k",
    "memory_mapping"        : "-M",
    "minimizer_len"         : "-l",
    "mininum_base_quality"  : "-Q",
    "mininum_hit_groups"    : "-g",
    "output"                : "-O",
    "paired"                : "-P",
    "protein"               : "-X",
    "quick"                 : "-q",
    "report"                : "-R",
    "report_minimizer_data" : "-K",
    "report_zero_counts"    : "-z",
    "sub_block_size"        : "-b",
    "threads"               : "-p",
    "unclassified_out"      : "-U",
    "use_mpa_style"         : "-m",
    "use_names"             : "-n",
    "skip_counts"           : "-s",
    "use_mpa_style"         : "-m",
    "report_zero_counts"    : "-z",
}
SCRIPT_PATHNAME = None

def get_binary_options(binary_pathname):
    options = []
    proc = subprocess.Popen(binary_pathname, stderr=subprocess.PIPE)
    lines = proc.stderr.readlines()
    for line in lines:
        match = re.search(b"\s(-\w)\s", line)
        if not match:
            continue
        options.append(match.group(1).decode())
    return options

def wrapper_args_to_binary_args(opts, argv, binary_args):
    for k, v in vars(opts).items():
        if k not in WRAPPER_ARGS_TO_BIN_ARGS:
            continue
        if WRAPPER_ARGS_TO_BIN_ARGS[k] not in binary_args:
            continue
        if v == False:
            continue
        if v is True:
            argv.append(WRAPPER_ARGS_TO_BIN_ARGS[k])
        else:
            argv.extend([WRAPPER_ARGS_TO_BIN_ARGS[k], str(v)])

def find_kraken2_binary(name):
    binary = None
    script_parent_directory = get_parent_directory(SCRIPT_PATHNAME)
    if os.path.exists(os.path.join(script_parent_directory, name)):
        binary = os.path.join(script_parent_directory, name)
    if binary == None:
        project_root = get_parent_directory(script_parent_directory)
        if 'src' in os.listdir(project_root) and \
           name in os.listdir(os.path.join(project_root, 'src')):
            binary = os.path.join(project_root, os.path.join("src", name))
    if not binary:
        sys.stderr.write("Unable to find {:s}, exiting".format(name))
        exit(1)
    return binary

def get_parent_directory(pathname):
    if len(pathname) == 0:
        return None
    pathname = os.path.abspath(pathname)
    if len(pathname) > 1 and pathname[-1] == os.path.sep:
        return os.path.dirname(pathname[:-1])
    return os.path.dirname(pathname)

def find_database(database_name):
    database_path = None
    if database_name.find(os.path.sep) < 0:
        if "KRAKEN2_DB_PATH" in os.environ:
            for directory in os.environ["KRAKEN2_DB_PATH"].split(":"):
                if os.path.exists(os.path.join(directory, database_name)):
                    database_path = os.path.join(directory, database_name)
                    break
        else:
            if database_name in os.listdir(os.getcwd()):
                database_path = database_name
    elif os.path.exists(database_name):
        database_path = database_name
    if database_path:
        for db_file in ['taxo.k2d', 'hash.k2d', 'opts.k2d']:
            if not os.path.exists(os.path.join(database_path, db_file)):
                return None
    return database_path

def remove_files(*filepaths):
    for fname in filepaths:
        if os.path.isdir(fname):
            shutil.rmtree(fname)
        else:
            os.remove(fname)

def check_seqid(seqid):
    taxid = None
    match = re.match("(?:^|\|)kraken:taxid\|(\d+)", seqid)
    if match:
        taxid = match.group(1)
    elif re.match("^\d+$", seqid):
        taxid = seqid
    if not taxid:
        match = re.match("(?:^|\|)([A-Z]+_?[A-Z0-9]+)(?:\||\b|\.)", seqid)
        if match:
            taxid = match.group(1)
    return taxid

def scan_fasta_file(in_file, out_file, lenient = False):
    for line in in_file:
        if not line.startswith(">"):
            continue
        for match in re.finditer("(?:^>|\x01)(\S+)", line):
            seqid = match.group(1)
            taxid = check_seqid(seqid)
            if not taxid:
                if lenient:
                    continue
                else:
                    exit(1)
            if re.match("^\d+$", taxid):
                out_file.write("TAXID\t{:s}\t{:s}\n".format(seqid, taxid))
            else:
                out_file.write("ACCNUM\t{:s}\t{:s}\n".format(seqid, taxid))

def lookup_accession_numbers(lookup_list_file, out_filename,  *accession_map_files):
    target_lists = {}
    with open(lookup_list_file, "r") as f:
        for line in f:
            line = line.strip()
            seqid, acc_num = line.split("\t")
            if acc_num in target_lists:
                target_lists[acc_num].append(seqid)
            else:
                target_lists[acc_num] = []
    initial_target_count = len(target_lists)
    with open(out_filename, "a") as out_file:
        for filename in accession_map_fs:
            with open(filename, "r") as in_file:
                in_file.readline()        # discard header line
                for line in in_file:
                    line = line.strip()
                    accession, with_version, taxid, gi = line.split("\t")
                    if accession in target_lists:
                        lst = target_lists[accession]
                        del target_lists[accession]
                        for seqid in lst:
                            out_file.write(seqid + "\t" + taxid + "\n")
                        if len(target_lists) == 0:
                            break
            if len(target_lists) == 0:
                break
    if target_lists:
        with open("unmapped.txt", "w") as f:
            for k in target_lists:
                f.write(k + "\n")

def cp_into_temp_file(old_file, temp_file):
    shutil.copy(old_file, temp_file)

def add_to_library(args):
    if not os.path.isdir(args.db):
        sys.stderr.write("Invalid database: {:s}\n".format(args.db))
        exit(1)
    library_pathname = os.path.join(args.db, "library")
    added_pathname = os.path.join(library_pathname, "added")
    os.makedirs(added_pathname, exist_ok=True)
    #os.chdir(added_pathname)
    with tempfile.NamedTemporaryFile(prefix="prelim_map_", suffix=".txt", dir=added_pathname, mode="w", delete=False) as out_file:
        with open(args.file, "r") as in_file:
            scan_fasta_file(in_file, out_file, lenient=True)
    with tempfile.NamedTemporaryFile(suffix =".fna", dir=added_pathname, mode="w", delete=False) as out_file:
        cp_into_temp_file(args.file, out_file.name)
    sys.stderr.write("Added " + args.file + " to library " + args.db + "\n")

def make_manifest_from_assembly_summary(assembly_summary_file, is_protein = False):
    suffix = "_protein.faa.gz" if is_protein else "_genomic.fna.gz"
    manifest_to_taxid = {}
    for line in assembly_summary_file:
        if line.startswith("#"):
            continue
        fields = line.strip().split("\t")
        taxid, asm_level, ftp_path = fields[5], fields[11], fields[19]
        if not re.match("Complete\ Genome|Chromosome", asm_level):
            continue
        if ftp_path == "na":
            continue
        remote_path = ftp_path + "/" + os.path.basename(ftp_path) + suffix
        try:
            # with urllib.request.urlopen(remote_path) as f:
            local_path = urllib.parse.urlsplit(remote_path).path.replace("/genomes/", "")
            manifest_to_taxid[local_path] = taxid
        except urllib.error.URLError:
            sys.stderr.write("{:s} does not exist on server, and will not be added to library\n".format(remote_path))
            continue
    with open("manifest.txt", "w") as f:
        for k in manifest_to_taxid:
            f.write(k + "\n")
    return manifest_to_taxid

def assign_taxid_to_sequences(manifest_to_taxid, is_protein):
    out_filename = "library.faa" if is_protein else "library.fna"
    with open(out_filename, "wb") as f:
        projects_added = 0
        total_projects = len(manifest_to_taxid)
        sequences_added = 0
        ch_added = 0
        ch = "aa" if is_protein else "bp"
        max_out_chars = 0
        for filepath in manifest_to_taxid:
            taxid = manifest_to_taxid[filepath]
            with gzip.open(filepath) as in_file:
                while True:
                    line = in_file.readline()
                    if line == b'':
                        break
                    if line.startswith(b">"):
                        line = line.replace(b">", str.encode(">kraken:taxid|"+taxid+"|"), 1)
                        sequences_added += 1
                    else:
                        ch_added += len(line) - 1
                    f.write(line)
            # remove_files(in_filename)
            projects_added += 1
            out_line = progress_line(projects_added, total_projects,
                                     sequences_added, ch_added, ch)
            max_out_chars = max(len(out_line), max_out_chars)
            space_line = " " * max_out_chars
            sys.stderr.write("\r{:s}\r{:s}".format(space_line, out_line))
        sys.stderr.write("\nAll files processed, cleaning up extra sequence files...")
        # remove_files("all")
        sys.stderr.write("done\n")

def progress_line(projects, total_projects, seqs, chars, ch):
    line = "Processed "
    if projects == total_projects:
        line += str(projects)
    else:
        line += "{:d}/{:d}".format(projects, total_projects)
    line += " project(s) {:d} sequence(s), ".format(total_projects, seqs)
    prefix = None
    for p in ["k", "M", "G", "T", "P", "E"]:
        if chars >= 1000:
            prefix = p
            chars /= 1000
        else:
            break
    if prefix:
        line += "{:.2f} {:s}{:s}".format(chars, prefix, ch)
    else:
        line += "{:f} {:s}".format(chars, ch)
    return line

def uncompress_file(compressed_filename, out_filename=None, buf_size=1024):
    open_mode = "ab"
    if not out_filename:
        out_filename, ext = os.path.splitext(compressed_filename)
        if os.path.exists(out_filename+"_tmp"):
            os.remove(out_filename+"_tmp")
        open_mode = "wb"
    with gzip.open(compressed_filename) as gz:
        with open(out_filename+"_tmp", open_mode) as f:
            sys.stderr.write("Decompressing {:s} into {:s}...".format(os.path.join(os.getcwd(), compressed_filename), os.path.join(os.getcwd(), out_filename)))
            while True:
                data = gz.read(buf_size)
                f.write(data)
                if data == b'':
                    break
            sys.stderr.write("done\n")
    os.rename(out_filename+"_tmp", out_filename)

def download_log(filename):
    cur_size = 0
    def inner(b, b_sz, tot):
        nonlocal cur_size, filename
        cur_size += b_sz
        if cur_size > tot:
            cur_size = tot
        sys.stderr.write("Downloading {:s} ... {:d}/{:d}\r".format(filename, cur_size, tot))
    return inner

def download_file(url, local_name=None):
    if not local_name:
        local_name = urllib.parse.urlparse(url).path.split("/")[-1]
    else:
        os.makedirs(os.path.dirname(local_name), exist_ok = True)
    urllib.request.urlretrieve(url, local_name,
                               reporthook = download_log(local_name))
    file_size = os.stat(local_name).st_size
    sys.stderr.write("Downloaded {:s} [{:d}] to {:s}\n".format(local_name,
                                                               file_size,
                                                               os.getcwd()))

def make_manifest_filter(file, regex):
    def inner(listing):
        nonlocal file, regex
        path = listing.split()[-1]
        if path.endswith(regex):
            file.write(path + "\n")
    return inner

def get_manifest(server, remote_path, regex):
    with open("manifest.txt", "w") as f:
        ftp = ftplib.FTP(server)
        ftp.login()
        ftp.cwd(remote_path)
        ftp.retrlines('LIST', callback=make_manifest_filter(f, regex))
        ftp.close()

def download_files_from_manifest(server, remote_dir, manifest_filename="manifest.txt", uncompress = False, out_filename = None):
    with open(manifest_filename, "r") as f:
        for filename in f:
            url = server + remote_dir + filename
            download_from_ftp(NCBI_SERVER, remote_dir, filename.strip())
            if uncompress:
                uncompress_file(filename.strip(), out_filename)

def download_taxonomy(args):
    taxonomy_path = os.path.join(args.db, "taxonomy")
    os.makedirs(taxonomy_path, exist_ok = True)
    os.chdir(taxonomy_path)
    if not args.skip_maps:
        if not args.protein:
            for subsection in ["gb", "wgs"]:
                sys.stderr.write("Downloading nucleotide {:s} accession to taxon map...".format(subsection))
                filename = "nucl_" + subsection + ".accession2taxid.gz"
                download_from_ftp(NCBI_SERVER, "pub/taxonomy/accession2taxid/", filename)
        else:
            sys.stderr.write("Downloading protein accession to taxon map\n")
            download_from_ftp(NCBI_SERVER, "pub/taxonomy/accession2taxid", "prot.accession2taxid.gz")
    sys.stderr.write("Downloaded accession to taxon map(s)\n")
    download_from_ftp(NCBI_SERVER, "pub/taxonomy", "taxdump.tar.gz")
    for filename in glob.glob("*accession2taxid.gz"):
        uncompress_file(filename)
    with tarfile.open("taxdump.tar.gz", "r:gz") as tar:
        tar.extractall()
    # TODO: delete files

def download_genomic_library(args):
    library_filename = "library.faa" if args.protein else "library.fna"
    library_pathname = os.path.join(args.db, "library")

    if args.library in ["achaea", "bacteria", "viral", "fungi", "plant", "human", "protozoa"]:
        library_pathname = os.path.join(library_pathname, args.library)
        os.makedirs(library_pathname, exist_ok = True)
        os.chdir(library_pathname)
        try:
            os.remove("assembly_summary.txt")
        except FileNotFoundError:
            pass
        remote_dir_name = args.library
        if args.library == "human":
            remote_dir_name = "vertebrate_mammalian/Homo_sapiens"
        try:
            url = "ftp://{:s}/genomes/refseq/{:s}/assembly_summary.txt".format(NCBI_SERVER, remote_dir_name)
            download_file(url)
        except urllib.error.URLError:
            sys.stderr.write("Error downloading assembly summary file for {:s}, exiting".format(args.library))
            exit(1)
        if args.library == "human":
            with open("assembly_summary.txt", "r") as f1:
                with open("grc.txt", "w") as f2:
                    for line in f1:
                        if line.find("Genome Reference Consortium"):
                            f2.write(line)
            os.rename("grc.txt", "assembly_summary.txt")
            # try:
        with open("assembly_summary.txt", "r") as f:
            filepath_to_taxid_table = make_manifest_from_assembly_summary(f, args.protein)
            sys.stderr.write("Step 1/2: Downloading files from ftp\n")
            download_files_from_manifest(FTP_SERVER, "/genomes/")
            sys.stderr.write("Step 2/2: Assigning taxonomic IDs to sequences\n")
            assign_taxid_to_sequences(filepath_to_taxid_table, args.protein)
        with open(library_filename, "r") as in_file:
            with open("prelim_map.txt", "a") as out_file:
                scan_fasta_file(in_file, out_file)
        # except:
        #     sys.stderr.write("something went wrong\n")
        #     exit(1)
        # files_to_remove = glob.glob("library.f*")
        files_to_remove = ["all", "manifest.txt"]
        remove_files(*files_to_remove)
    elif args.library == "plasmid":
        library_pathname = os.path.join(library_pathname, args.library)
        os.makedirs(library_pathname, exist_ok = True)
        os.chdir(library_pathname)
        files_to_remove = glob.glob("library.f*").extend(glob.glob("plasmid.*"))
        sys.stderr.write("Downloading plasmid files from FTP...")
        pat = ".faa.gz" if args.protein else ".fna.gz"
        get_manifest(NCBI_SERVER, "genomes/refseq/plasmid/", pat)
        download_files_from_manifest(FTP_SERVER, "/genomes/refseq/plasmid/", uncompress = True, out_filename = library_filename)
        with open(library_filename, "r") as in_file:
            with open("prelim_map.txt", "w") as out_file:
                scan_fasta_file(in_file, out_file)
        remove_files(*files_to_remove)
        sys.stderr.write("done\n")
    elif args.library in ["nr", "nt"]:
        protein_lib = True if args.library == "nr" else False
        if protein_lib and not args.protein:
            sys.stderr.write("{:s} is a protein database, and the Kraken2 database specified is nucleotide".format(args.library))
            exit(1)
        library_pathname = os.path.join(library_pathname, args.library)
        os.makedirs(library_pathname, exist_ok=True)
        os.chdir(library_pathname)
        download_from_ftp(NCBI_SERVER, "blast/db/FASTA/", args.library + ".gz")
        with open(library_filename, "r") as in_file:
            with open("prelim_map.txt", "w") as out_file:
                scan_fasta_file(in_file, out_file, lenient=True)
    elif args.library in ["UniVec", "UniVec_Core"]:
        if args.protein:
            sys.stderr.write("{:s} is for nucleotide databases only\n".format(args.library))
            exit(1)
        library_pathname = os.path.join(library_pathname, args.library)
        os.makedirs(library_pathname, exist_ok=True)
        os.chdir(library_pathname)
        download_from_ftp(NCBI_SERVER, "pub/UniVec", args.library)
        special_taxid = 28384
        sys.stderr.write("Adding taxonomy ID of {:d} to all sequences...".format(special_taxid))
        with open(args.library, "r") as in_file:
            with open("library.fna", "w") as out_file:
                for line in in_file:
                    if line.startswith(">"):
                        line = re.sub(">", ">kraken:taxid|"+ str(special_taxid) + "|", line)
                    out_file.write(line)
        with open("library.fna", "r") as in_file:
            with open("prelim_map.txt", "w") as out_file:
                scan_fasta_file(in_file, out_file)
        sys.stderr.write("done\n")

# def get_mod_date(ftp, filename):
def get_abs_path(filename):
    return os.path.abspath(filename)

def ftp_download_callback(f, size_on_disk, remote_size):
    def inner(block):
        nonlocal size_on_disk, f, remote_size
        f.write(block)
        size_on_disk += len(block)
        if size_on_disk > remote_size:
            size_on_disk = remote_size
        sys.stderr.write("Downloading {:s} ... {:d}/{:d}\r".format(os.path.basename(f.name), size_on_disk, remote_size))
    return inner

def download_from_ftp(server, remote_dir, filepath):
    rest = None
    if os.path.exists(filepath):
        rest = os.stat(filepath).st_size
        if rest == 0: rest = None
    else:
        if os.path.basename(filepath) != filepath:
            os.makedirs(os.path.dirname(filepath), exist_ok = True)
    ftp = ftplib.FTP(server)
    ftp.login()
    ftp.cwd(remote_dir)
    if rest == ftp.size(filepath):
            sys.stderr.write("Skipping {:s}\n".format(get_abs_path(filepath)))
            return
    filename = os.path.basename(filepath)
    with open(filepath, "ab") as f:
        if rest != None and rest > 0:
            f.seek(rest)
        # print("local file size = {:d}".format(rest))
        if rest != None:
            sys.stderr.write("Resuming download of partial file {:s}\n".format(filepath))
        cb = ftp_download_callback(f, rest or 0, ftp.size(filepath))
        ftp.retrbinary('RETR ' + filepath, cb, rest = f.tell())
        sys.stderr.write("Downloaded {:s} [{:d}] to {:s}\n".format(filename, ftp.size(filepath), os.getcwd()))
        # listing = io.StringIO()
        # ftp.retrlines('LIST ' + filepath, callback = listing.write)
        # print(listing.getvalue().split()[5:8])
    ftp.quit()

def build_kraken2_db(args):
    default_aa_minimizer_length = 12
    default_aa_kmer_length = 15
    default_aa_minimizer_spaces = 0
    default_nt_minimize_length = 31
    default_nt_kmer_length = 35
    default_nt_minimizer_spaces = 35

    if not args.kmer_len:
        args.kmer_len = default_aa_kmer_length if args.protein else default_nt_kmer_length
    if not args.minimizer_len:
        args.minimizer_len = default_aa_minimizer_length if args.protein else default_nt_minimize_length
    if not os.path.isdir(get_abs_path(args.db)):
        sys.stderr.write("Cannot find Kraken 2 database: \"{:s}\n".format(args.db))
        exit(1)
    os.chdir(args.db)
    if not os.path.isdir("taxonomy"):
        sys.stderr.write("Cannot find taxonomy subdirectory in database\n")
        exit(1)
    if not os.path.isdir("library"):
        sys.stderr.write("Cannot find library subdirectory in database\n")
        exit(1)
    if os.path.isdir(os.path.join("library", "added")):
        added_dirpath = os.path.join("library", "added")
        prelim_map_filenames = glob.glob(os.path.join(added_dirpath, "prelim_map_*.txt"))
        if prelim_map_filenames:
            with open(os.path.join(added_dirpath, "prelim_map.txt"), "w") as out_file:
                for filename in prelim_map_filenames:
                    with open(filename, "r") as in_file:
                        for line in in_file:
                            out_file.write(line)
    if os.path.isfile("seqid2taxid.map"):
        sys.stderr.write("Sequence ID to taxonomy ID map already, skipping\n")
    else:
        sys.stderr.write("Creating sequence ID to taxonomy ID map...")
        with open(os.path.join("taxonomy", "prelim_map.txt"), "w") as out_file:
            depth = 1
            for dirpath, dirnames, filenames in os.walk("library"):
                if "prelim_map.txt" in filenames:
                    for line in open(os.path.join(os.path.abspath(dirpath), "prelim_map.txt"), "r"):
                        out_file.write(line)
                #     depth += 1
                # if depth > 2:
                #     break
        if os.path.getsize(os.path.join("taxonomy", "prelim_map.txt")) == 0:
            os.remove(os.path.join("taxonomy", "prelim_map.txt"))
            sys.stderr.write("No preliminary seqid/taxid mapping files found, aborting\n")
            exit(1)
        with open(os.path.join("taxonomy", "prelim_map.txt"), "r") as in_file:
            with open("seqid2taxid.map.tmp", "w") as seqid2taxid_file:
                with open("accmap.tmp", "w") as accmap_file:
                    for line in in_file:
                        new_line = "\t".join(line.split("\t")[1:])
                        if line.startswith("TAXID"):
                            seqid2taxid_file.write(new_line)
                        elif line.startswith("ACCNUM"):
                            accmap_file.write(new_line)
        if os.path.getsize("accmap.tmp") > 0:
            accession2taxid_filenames = glob.glob("taxonomy/*.accession2taxid")
            if accession2taxid_filenames:
                lookup_accession_numbers("accmap.tmp", "seqid2taxid.map.tmp", *accession2taxid_filenames)
            else:
                sys.stderr.write("Accession to taxid map files are required to build this database.\n")
                sys.stderr.write("Run kraken2 build --db {:s} --download-taxonomy again".format(args.db))
                exit(1)
        os.remove("accmap.tmp")
        move("seqid2taxid.map.tmp", "seqid2taxid.map")
        sys.stderr.write("done\n")
    sys.stderr.write("Estimating required capacity...")
    estimate_capacity_binary = find_kraken2_binary("estimate_capacity")
    argv = [estimate_capacity_binary]
    wrapper_args_to_binary_args(args, argv, get_binary_options(estimate_capacity_binary))
    fasta_filenames = glob.glob(os.path.join("library", os.path.join("**", "*.f[an]a")), recursive=True)
    argv.extend(fasta_filenames)
    sys.stderr.write("Estimating hash table size.\n")
    proc = subprocess.Popen(argv, stdin=subprocess.PIPE, stdout=subprocess.PIPE)
    # for filename in glob.glob(os.path.join("library", "*.f[an]a")):
    #     with open(filename, "rb") as in_file:
    #         for line in in_file:
    #             proc.stdin.write(line)
    estimate = proc.communicate()[0].decode()
    required_capacity = (int(estimate.strip()) + 8192) / args.load_factor
    sys.stderr.write("Estimated hash table requirement: {:f} bytes\n".format(required_capacity * 4))
    if args.max_db_size:
        if args.max_db_size < required_capacity * 4:
            args.max_db_size /= 4
            sys.stderr.write("Specifiying lower maximum hash table size of {:f}".format(args.max_db_size))
    if os.path.isfile("hash.k2d"):
        sys.stderr.write("Hash table already present, skipping build\n")
    else:
        sys.stderr.write("Building database ...")
        build_db_bin = find_kraken2_binary("build_db")
        argv = [build_db_bin, "-H", "hash.k2d.tmp", "-t", "taxo.k2d.tmp",
                "-o", "opts.k2d.tmp", "-n", "taxonomy", "-m", "seqid2taxid.map", "-c", str(required_capacity)]
        wrapper_args_to_binary_args(args, argv, get_binary_options(build_db_bin))
        argv.extend(fasta_filenames)
        r = subprocess.call(argv)
        # for filename in glob.glob(os.path.join("library", "*.f[an]a")):
        #     with open(filename, "rb") as in_file:
        #         for line in in_file:
        #             proc.stdin.write(line)
        move("hash.k2d.tmp", "hash.k2d")
        move("taxo.k2d.tmp", "taxo.k2d")
        move("opts.k2d.tmp", "opts.k2d")
        sys.stderr.write("done\n")

def build_rdp_taxonomy(f):
    seqid_map = {}
    seen_it = {}
    child_data = {"root;no rank": {}}

    for line in f:
        if not line.startswith(">"):
            continue
        line = line.strip()
        seq_label, taxonomy_string = line.split("\t")
        seqid = seq_label.split(" ")[0]
        taxonomy_string = re.sub("^Lineage=Root;rootrank;", "root;no rank;", taxonomy_string)
        taxonomy_string = re.sub(";$", ";no rank", taxonomy_string)
        seqid_map[seqid] = taxonomy_string
        seen_it.setdefault(taxonomy_string, 0)
        seen_it[taxonomy_string] += 1
        if seen_it[taxonomy_string] > 1:
            continue
        while True:
            match = re.search("(;[^;]+;[^;]+)$", taxonomy_string)
            if match == None:
                break
            level = match.group(1)
            taxonomy_string = re.sub(";[^;]+;[^;]+$", "", taxonomy_string)
            key = taxonomy_string + level
            child_data.setdefault(taxonomy_string, {})
            seen_it.setdefault(taxonomy_string, 0)
            child_data[taxonomy_string].setdefault(key, 0)
            child_data[taxonomy_string][key] += 1
            seen_it[taxonomy_string] += 1
            if seen_it[taxonomy_string] > 1:
                break
    id_map = {}
    next_node_id = 1
    with open("names.dmp", "w") as names_file:
        with open("nodes.dmp", "w") as nodes_file:
            bfs_queue = [["root;no rank", 1]]
            while len(bfs_queue) > 0:
                node, parent_id = bfs_queue.pop()
                match = re.search("([^;]+);([^;]+)$", node)
                if match == None:
                    sys.stderr.write("BFS processing encountered formatting eror, \"{:s}\"\n".format(node))
                    exit(1)
                display_name, rank = match.group(1), match.group(2)
                if rank == "domain":
                    rank = "superkingdom"
                node_id, next_node_id = next_node_id, next_node_id + 1
                id_map[node] = node_id
                names_file.write("{:d}\t|\t{:s}\t|\t-\t|\tscientific name\t|\n".format(node_id, display_name))
                nodes_file.write("{:d}\t|\t{:d}\t|\t{:s}\t|\t-\t|\n".format(node_id, parent_id, rank))
                children = sorted([key for key in child_data[node]]) if node in child_data else []
                for node in children:
                    bfs_queue.insert(0, [node, node_id])
    with open("seqid2taxid.map", "w") as f:
        for seqid in sorted([key for key in seqid_map]):
            taxid = id_map[seqid_map[seqid]]
            f.write("{:s}\t{:d}\n".format(seqid, taxid))

def move(src, dst):
    src = os.path.abspath(src)
    dst = os.path.abspath(dst)
    if os.path.isfile(src) and os.path.isdir(dst):
        dst = os.path.join(dst, os.path.basename(src))
    shutil.move(src, dst)

def build_standard_database(args):
    download_taxonomy(args)
    for library in ["archaea", "bacteria", "viral", "plasmid", "human", "UniVec_Core"]:
        if library == "UniVec_Core" and args.protein:
            continue
        args.library = library
        download_genomic_library(args)
    build_kraken2_db(args)

def build_silva_taxonomy(in_file):
    id_map = {"root": 1}
    with open("names.dmp", "w") as names_file:
        with open("nodes.dmp", "w") as nodes_file:
            names_file.write("1\t|\troot\t|\t-\t|\tscientific name\t|\n")
            nodes_file.write("1\t|\t1\t|\tno rank\t|\t-\t|\n")
            for line in in_file:
                line = line.strip()
                taxonomy_string, node_id, rank = line.split("\t")[:3]
                id_map[taxonomy_string] = node_id
                match = re.search("^(.+;|)([^;]+);$", taxonomy_string)
                if match:
                    parent_name = match.group(1)
                    display_name = match.group(2)
                    if parent_name == "":
                        parent_name = "root"
                    parent_id = id_map[parent_name] or None
                    if not parent_id:
                        sys.stderr.write("orphan error: \"{:s}\"\n".format(line))
                        exit(1)
                    if rank == "domain":
                        rank = "superkingdom"
                    names_file.write("{:s}\t|\t{:s}\t|\t-\t|\tscientific name\t|\n".format(node_id, display_name))
                    nodes_file.write("{:s}\t|\t{:s}\t|\t{:s}\t|\t-\t|\n".format(node_id, str(parent_id), rank))
                else:
                    sys.stderr.write("strange input: \"{:s}\"\n".format(line))
                    exit(1)

def install_16S_silva(args):
    os.makedirs(args.db, exist_ok = True)
    os.chdir(args.db)
    for directory in ["data", "taxonomy", "library"]:
        os.makedirs(directory, exist_ok=True)
    os.chdir("data")
    ftp_server = "ftp.arb-silva.de"
    remote_directory = "release_138_1/Exports"
    fasta_filename = "SILVA_138.1_SSURef_NR99_tax_silva.fasta.gz"
    taxonomy_prefix = "tax_slv_ssu_138.1"
    download_from_ftp(ftp_server, remote_directory, fasta_filename)
    download_from_ftp(ftp_server, remote_directory + "/taxonomy", taxonomy_prefix + ".acc_taxid.gz")
    uncompress_file(taxonomy_prefix + ".acc_taxid.gz")
    download_from_ftp(ftp_server, remote_directory + "/taxonomy", taxonomy_prefix + ".txt.gz")
    with gzip.open(taxonomy_prefix + ".txt.gz", "rt") as f:
        build_silva_taxonomy(f)
    os.chdir(os.path.pardir)
    move(os.path.join("data", "names.dmp"), "taxonomy")
    move(os.path.join("data", "nodes.dmp"), "taxonomy")
    move(os.path.join("data", taxonomy_prefix + ".acc_taxid"), "seqid2taxid.map")
    with gzip.open(os.path.join("data", fasta_filename), "rt") as in_file:
        with open(os.path.join("library", "silva.fna"), "w") as out_file:
            for line in in_file:
                if not line.startswith(">"):
                    line = line.replace("U", "T")
                out_file.write(line)
    os.chdir(os.path.pardir)
    build_kraken2_db(args)

def build_gg_taxonomy(in_file):
    rank_codes = {
        "k": "superkingdom",
        "p": "phylum",
        "c": "class",
        "o": "order",
        "f": "family",
        "g": "genus",
        "s": "species"
    }
    seqid_map = {}
    seen_it = {}
    child_data = {"root": {}}
    for line in in_file:
        line = line.strip()
        seqid, taxonomy_string = line.split("\t")
        taxonomy_string = re.sub("(; [a-z]__)+$", "", taxonomy_string)
        seqid_map[seqid] = taxonomy_string
        seen_it.setdefault(taxonomy_string, 0)
        seen_it[taxonomy_string] += 1
        if seen_it[taxonomy_string] > 1:
            continue
        while True:
            match = re.search("(; [a-z]__[^;]+$)", taxonomy_string)
            if not match:
                break
            level = match.group(1)
            taxonomy_string = re.sub("(; [a-z]__[^;]+$)", "", taxonomy_string)
            child_data.setdefault(taxonomy_string, {})
            key = taxonomy_string + level
            seen_it.setdefault(taxonomy_string, 0)
            child_data[taxonomy_string].setdefault(key, 0)
            child_data[taxonomy_string][key] += 1
            seen_it[taxonomy_string] += 1
            if seen_it[taxonomy_string] > 1:
                break
        if seen_it[taxonomy_string] == 1:
            child_data["root"].setdefault(taxonomy_string, 0)
            child_data["root"][taxonomy_string] += 1
    id_map = {}
    next_node_id = 1
    with open("names.dmp", "w") as names_file:
        with open("nodes.dmp", "w") as nodes_file:
            bfs_queue = [["root", 1]]
            while len(bfs_queue) > 0:
                node, parent_id = bfs_queue.pop()
                display_name = node
                rank = None
                match = re.search("g__([^;]+); s__([^;]+)$", node)
                if match:
                    genus, species = match.group(1), match.group(2)
                    rank = "species"
                    if re.search(" endosymbiont ", species):
                        display_name = species
                    else:
                        display_name = genus + " " + species
                else:
                    match = re.search("([a-z])__([^;]+)$", node)
                    if match:
                        rank = rank_codes[match.group(1)]
                        display_name = match.group(2)
                rank = rank or "no rank"
                node_id, next_node_id = next_node_id, next_node_id + 1
                id_map[node] = node_id
                names_file.write("{:d}\t|\t{:s}\t|\t-\t|\tscientific name\t|\n".format(node_id, display_name))
                nodes_file.write("{:d}\t|\t{:d}\t|\t{:s}\t|\t-\t|\n".format(node_id, parent_id, rank))
                children = sorted([key for key in child_data[node]]) if node in child_data else []
                for node in children:
                    bfs_queue.insert(0, [node, node_id])
    with open("seqid2taxid.map", "w") as f:
        for seqid in sorted([key for key in seqid_map], key = int):
            taxid = id_map[seqid_map[seqid]]
            f.write("{:s}\t{:d}\n".format(seqid, taxid))

def install_16S_gg(args):
    os.makedirs(args.db, exist_ok = True)
    ftp_server = "greengenes.microbio.me"
    gg_version = "gg_13_5"
    remote_directory = "greengenes_release/" + gg_version
    os.chdir(args.db)
    for directory in ["data", "taxonomy", "library"]:
        os.makedirs(directory, exist_ok = True)
    os.chdir("data")
    download_from_ftp(ftp_server, remote_directory, gg_version + ".fasta.gz")
    uncompress_file(gg_version + ".fasta.gz")
    download_from_ftp(ftp_server, remote_directory, gg_version + "_taxonomy.txt.gz")
    uncompress_file(gg_version + "_taxonomy.txt.gz")
    with open(gg_version + "_taxonomy.txt", "r") as f:
        build_gg_taxonomy(f)
    os.chdir(os.path.abspath(os.path.pardir))
    move(os.path.join("data", "names.dmp"), "taxonomy")
    move(os.path.join("data", "nodes.dmp"), "taxonomy")
    move(os.path.join("data", "seqid2taxid.map"), os.getcwd())
    move(os.path.join("data", gg_version + ".fasta"), os.path.join("library", "gg.fna"))
    os.chdir(os.path.abspath(os.path.pardir))
    build_kraken2_db(args)

def install_16S_rdp(args):
    os.makedirs(args.db, exist_ok = True)
    os.chdir(args.db)
    for directory in ["data", "taxonomy", "library"]:
        os.makedirs(directory, exist_ok = True)
    os.chdir("data")
    download_file("http://rdp.cme.msu.edu/download/current_Bacteria_unaligned.fa.gz")
    download_file("http://rdp.cme.msu.edu/download/current_Archaea_unaligned.fa.gz")
    for gz_file in glob.glob("*.gz"):
        uncompress_file(gz_file)
        os.remove(gz_file)
    for filename in glob.glob("current_*_unaligned.fa"):
        with open(filename, "r") as f:
            build_rdp_taxonomy(f)
    os.chdir(os.pardir)
    move(os.path.join("data", "names.dmp"), "taxonomy")
    move(os.path.join("data", "nodes.dmp"), "taxonomy")
    move(os.path.join("data", "seqid2taxid.map"), os.getcwd())
    for filename in glob.glob(os.path.join("data", "*.fa")):
        new_filename = os.path.basename(re.sub("\.fa$", ".fna", filename))
        shutil.move(filename, os.path.join("library", new_filename))
    build_kraken2_db(args)

def make_seqid_to_taxid_map(in_file, quiet, accession_map_files = False, library_filename = None):
    target_lists = {}
    for line in in_file:
        match = re.match(">(\S+)", line)
        if match == None:
            continue
        seqid = match.group(1)
        output = None
        regexes = ["(?:^|\|)kraken:taxid\|(\d+)",
                   "^\d+$",
                   "(?:^|\|)([A-Z]+_?[A-Z0-9]+)(?:\||\b|\.)",
                   "(?:^|\|)gi\|(\d+)"]
        match = None
        index = None
        for i, regex in enumerate(regexes):
            match = re.match(regex, seqid)
            if match:
                index = i
                break
        if index == 0:
            output = seqid + "\t" + match.group(1) + "\n"
        elif index == 1:
            output = seqid + "\t" + seqid + "\n"
        elif index in [2, 3]:
            if not quiet:
                capture = match_group(1)
                target_lists.setdefault(capture, [])
                target_lists[capture].insert(0, seqid)
        else:
            sys.stderr.write("Unable to determine taxonomy ID for sequence {:s}\n".format(seqid))
            exit(1)
        if output and not quiet:
            print(output)
    if quiet:
        if len(target_lists) == 0:
            sys.stderr.write("External map required\n")
        exit(0)
    if len(target_lists) == 0:
        exit(0)
    if not accession_map_files and library_map_filename == None:
        sys.stderr.write("Found sequence ID without explicit taxonomy ID, but no map used\n")
        exit(1)
    if library_map_file:
        with open(library_map_filename, "r") as f:
            for line in f:
                line = line.strip()
                seqid, taxid = line.split("\t")
                if seqid in target_lists:
                    print("{:s}\t{:s}\n".format(seqid, taxid))
                    del(target_lists[seqid])
    if len(target_lists) == 0:
        exit(0)
    for filename in accesion_map_files:
        with open(filename, "r") as f:
            f.readline()
            for line in f:
                line = line.strip()
                accession, with_version, taxid, gi = line.split("\t")
                if accession in target_lists:
                    l = target_lists[accession]
                    del(target_lists[accession])
                    for seqid in l:
                        print("{:s}\t{:s}".format(seqid, taxid))
                if gi != "na" and gi in target_lists:
                    l = target_lists[gi]
                    del(target_lists[gi])
                    for seqid in l:
                        print("{:s}\t{:s}\n".format(seqid, taxid))

def classify(args):
    classify_bin = find_kraken2_binary("classify")
    if classify_bin == None:
        sys.stderr.write("Unable to find classify binary... exiting")
        exit(1)
    database_path = find_database(args.db)
    if database_path == None:
        sys.stderr.write("{:s} is not a valid database... exiting".format(args.db))
        exit(1)
    if "paired" in args and len(args.filenames) % 2 != 0:
        sys.stderr.write("--paired requires an even number of file names")
        exit(1)
    if args.confidence < 0 or args.confidence > 1:
        sys.stderr.write("--confidence, {:f}, must be between 0 and 1 inclusive".format(args.confidence))
        exit(1)
    argv = [classify_bin,
            "-H", os.path.join(database_path, "hash.k2d"),
            "-t", os.path.join(database_path, "taxo.k2d"),
            "-o", os.path.join(database_path, "opts.k2d")]
    wrapper_args_binary_args(args, argv, get_binary_options(classify_bin))
    argv.extend(args.filenames)
    sys.exit(subprocess.call(argv))

def inspect_db(args):
    database_pathname = find_database(args.db)
    if not database_pathname:
        sys.stderr.write("{:s} database does not exist\n")
        exit(1)
    for database_file in ["taxo.k2d", "hash.k2d", "opts.k2d"]:
        if not os.path.isfile(os.path.join(database_pathname, database_file)):
            sys.stderr.write("{:s} does not exist\n".format(database_file))
    dump_table_bin = find_kraken2_binary("dump_table")
    argv = [dump_table_bin,
            "-H", os.path.join(database_pathname, "hash.k2d"),
            "-t", os.path.join(database_pathname, "taxo.k2d"),
            "-o", os.path.join(database_pathname, "opts.k2d")]
    wrapper_args_to_binary_args(args, argv, get_binary_options(dump_table_bin))
    exit(subprocess.call(argv))

def format_bytes(size):
    prefix = "B"
    for p in ["kB", "MB", "GB", "TB", "PB", "EB"]:
        if size >= 1024:
            prefix = p
            size /= 1024
        else:
            break
    return "{:.2f} {:s}".format(size, prefix)

def clean_db(args):
    os.chdir(args.db)
    old_free = shutil.disk_usage(os.getcwd()).free
    for filename in ["library", "taxonomy", "seqid2taxid.map"]:
        if os.path.exists(filename):
            if os.path.isdir(filename):
                shutil.rmtree(filename)
            else:
                os.remove(filename)
    new_free = shutil.disk_usage(os.getcwd()).free
    freed = new_free - old_free
    sys.stderr.write("Cleaned up {:s} of space\n".format(format_bytes(freed)))

def make_build_parser(subparsers):
    parser = subparsers.add_parser("build",
                                   help="Create DB from library\
                                   (requires taxonomy downloaded and at least one file\
                                   in library")
    parser.add_argument("--db", type=str, required=True,
                        help="Name of Kraken 2 database")
    mutex_group = parser.add_mutually_exclusive_group()
    mutex_group.add_argument("--standard", action="store_true",
                             help="Make standard database")
    mutex_group.add_argument("--special", type=str,
                             choices=["greengenes", "rdp", "silva"],
                             help="Build special database")
    parser.add_argument("--kmer-len", type=int,
                        help="K-mer length in bp/aa")
    parser.add_argument("--minimizer-len", type=int,
                        help="Minimizer length in bp/aa")
    parser.add_argument("--threads", type=int,
                        default=os.environ.get("KRAKEN2_NUM_THREADS") or 1,
                        help="Number of threads")
    parser.add_argument("--load-factor", type=float, default=0.7,
                        help="Proportion of the hash table to be populated")
    parser.add_argument("--fast-build", action="store_true",
                        help="Do not require database to be deterministically\
                        built when using multiple threads. This is faster, but\
                        does introduce variability in minizer/LCA pairs.")
    parser.add_argument("--max-db-size", type=int,
                        help="Maximum number of bytes for Kraken 2 hash table;\
                        if the estimator determines more would normally be\
                        needed, the reference library will be downsampled to fit")
    parser.add_argument("--protein", action="store_true",
                        help="Build a protein database for translated search")
    parser.add_argument("--block-size", type=int, default=16384,
                        help="Read block size")
    parser.add_argument("--sub-block-size", type=int, default=0,
                        help="Read subblock size")

def make_download_taxonomy_parser(subparsers):
    parser = subparsers.add_parser("download-taxonomy",
                                   help="Download NCBI taxonomic information")
    parser.add_argument("--db", type=str, required=True,
                        help="Name of Kraken 2 database")
    parser.add_argument("--skip-maps", action="store_true",
                        help="Avoids downloading accession number to taxid maps")
    parser.add_argument("--protein", action="store_true",
                        help="Build a protein database for translated search")
    parser.add_argument("--no-masking", action="store_true",
                        help="Avoid asking low-complexity sequences prior to\
                        building; masking requires dustmasker or segmasker to be\
                        installed")

def make_download_library_parser(subparsers):
    parser = subparsers.add_parser("download-library",
                                   help="Download and build a special database")
    parser.add_argument("--db", type=str, required=True,
                        help="Name of Kraken 2 database")
    parser.add_argument("--library", type=str,
                        choices=["archaea", "bacteria", "plasmid",
                                 "viral", "human", "fungi", "plant",
                                 "protozoa", "nr", "nt", "UniVec", "UniVec_Core"],
                        help="Name of library to download"
)
    parser.add_argument("--protein", action="store_true",
                        help="Build a protein database for translated search")
    parser.add_argument("--no-masking", action="store_true",
                        help="Avoid asking low-complexity sequences prior to\
                        building; masking requires dustmasker or segmasker to be\
                        installed")

def make_add_to_library_parser(subparsers):
    parser = subparsers.add_parser("add-to-library",
                                   help="Add file to library")
    parser.add_argument("--db", type=str, required=True,
                        help="Name of Kraken 2 database")
    parser.add_argument("--file", type=str,
                        help="Pathname of file to be added to library")
    parser.add_argument("--no-masking", action="store_true",
                        help="Avoid asking low-complexity sequences prior to\
                        building; masking requires dustmasker or segmasker to be\
                        installed")

def make_classify_parser(subparsers):
    parser = subparsers.add_parser("classify",
                                   help="Run classifier")
    parser.add_argument("--db", type=str, required=True,
                        help="Name of Kraken2 DB")
    parser.add_argument("--threads", type=int,
                        default=os.environ.get("KRAKEN2_NUM_THREADS") or 1,
                        help="Number of threads")
    parser.add_argument("--quick", action='store_true', default=argparse.SUPPRESS,
                        help="Quick operation (use first hit or hits)")
    parser.add_argument("--unclassified-out", type=argparse.FileType('w'),
                        default=argparse.SUPPRESS,
                        help="Print unclassified sequences to filename")
    parser.add_argument("--classified-out", type=argparse.FileType('w'),
                        default=argparse.SUPPRESS,
                        help="Print classified sequences to filename")
    parser.add_argument("--output", type=argparse.FileType('w'),
                        default=argparse.SUPPRESS,
                        help="Print output to file (default: stdout) \"-\" will \
                        suppress normal output")
    parser.add_argument("--confidence", type=float,
                        default=0.0,
                        help="confidence score threshold (default: 0.0); must be in [0,1]")
    parser.add_argument("--mininum-base-quality", type=int, default=0,
                        help="Mininum base quality used in classification")
    parser.add_argument("--report", type=argparse.FileType('w'),
                        default=argparse.SUPPRESS,
                        help="Print a report with aggregate counts/clade to file")
    parser.add_argument("--use-mpa-style", action='store_true',
                        default=argparse.SUPPRESS,
                        help="With --report, format report output like Kraken 1's\
                        kraken-mpa-report")
    parser.add_argument("--report-zero-counts", action='store_true',
                        default=argparse.SUPPRESS,
                        help="With --report, report counts for ALL taxa, even if\
                        counts are zero")
    parser.add_argument("--report-minimizer-data", action='store_true',
                        default=argparse.SUPPRESS,
                        help="With --report, report minimizer and distinct minimizer\
                        count information in addition to normal Kraken report")
    parser.add_argument("--memory-mapping", action='store_true',
                        default=argparse.SUPPRESS,
                        help="Avoids loading database into RAM")
    parser.add_argument("--paired", action='store_true',
                        default=argparse.SUPPRESS,
                        help="The filenames provided have paired-end reads")
    parser.add_argument("--use-names", action='store_true',
                        default=argparse.SUPPRESS,
                        help="Print scientific names instead of just taxids")
    parser.add_argument("--mininum-hit-groups", type=int,
                        default=2,
                        help="Minimum number of hit groups (overlapping k-mers\
                        sharing the same minimizer) needed to make a call")
    parser.add_argument("filenames", nargs='+', type=str)

def make_inspect_parser(subparsers):
    parser = subparsers.add_parser("inspect",
                                   help="Inspect Kraken 2 database")
    parser.add_argument("--db", type=str, required=True,
                        help="Name of Kraken2 DB")
    parser.add_argument("--threads", type=int,
                        default=os.environ.get("KRAKEN2_NUM_THREADS") or 1,
                        help="Number of threads")
    parser.add_argument("--skip-counts", action="store_true",
                        help="Only print database summary statistics")
    parser.add_argument("--use-mpa-style", action="store_true",
                        help="Format output like Kraken 1's kraken-mpa-report")
    parser.add_argument("--report-zero-counts", action="store_true",
                        help="Report counts for ALL taxa, even if counts are zero")

def make_clean_parser(subparsers):
    parser = subparsers.add_parser("clean",
                                   help="Remove unneeded files from database")
    parser.add_argument("--db", type=str, required=True,
                        help="Name of Kraken2 DB")

def make_cmdline_parser():
    parser = argparse.ArgumentParser("kraken2")
    subparsers = parser.add_subparsers()
    make_add_to_library_parser(subparsers)
    make_download_library_parser(subparsers)
    make_download_taxonomy_parser(subparsers)
    make_build_parser(subparsers)
    make_classify_parser(subparsers)
    make_inspect_parser(subparsers)
    make_clean_parser(subparsers)
    return parser

def k2_main():
    global SCRIPT_PATHNAME
    SCRIPT_PATHNAME = os.path.realpath(inspect.getsourcefile(k2_main))

    parser = make_cmdline_parser()
    args = parser.parse_args(sys.argv[1:])
    task = sys.argv[1]
    if task not in ["classify", "inspect"]:
        args.db = os.path.abspath(args.db)
    if task == "download-taxonomy":
        download_taxonomy(args)
    elif task == "classify":
        classify(args)
    elif task == "download-library":
        download_genomic_library(args)
    elif task == "add-to-library":
        add_to_library(args)
    elif task == "build":
        if args.standard:
            pass
        elif args.special:
            if args.special == "greengenes":
                install_16S_gg(args)
            elif args.special == "silva":
                install_16S_silva(args)
            else:
                install_16S_rdp(args)
        else:
            build_kraken2_db(args)
    elif task == "inspect":
        inspect_db(args)
    elif task == "clean":
        clean_db(args)

if __name__ == '__main__':
    k2_main()
